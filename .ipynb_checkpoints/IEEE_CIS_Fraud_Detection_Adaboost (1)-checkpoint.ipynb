{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Transaction Table *\n",
    "- TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n",
    "- TransactionAMT: transaction payment amount in USD\n",
    "- ProductCD: product code, the product for each transaction\n",
    "- card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n",
    "- addr: address\n",
    "- dist: distance\n",
    "- P_ and (R__) emaildomain: purchaser and recipient email domain\n",
    "- C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n",
    "- D1-D15: timedelta, such as days between previous transaction, etc.\n",
    "- M1-M9: match, such as names on card and address, etc.\n",
    "- Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
    "\n",
    "Categorical Features:\n",
    "- ProductCD\n",
    "- card1 - card6\n",
    "- addr1, addr2\n",
    "- Pemaildomain Remaildomain\n",
    "- M1 - M9\n",
    "\n",
    "### Identity Table *\n",
    "Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions. \n",
    "They're collected by Vesta’s fraud protection system and digital security partners.\n",
    "(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n",
    "\n",
    "Categorical Features:\n",
    "- DeviceType\n",
    "- DeviceInfo\n",
    "- id12 - id38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided to exclude all Vxxx features from the analysis as we do not know what they are or how they were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T01:44:32.153275Z",
     "start_time": "2019-08-18T01:44:27.637345Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T01:46:26.236140Z",
     "start_time": "2019-08-18T01:44:32.158450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset_path = '../datasets/fraud_datasets/' # Local location\n",
    "# dataset_path = '../input/ieee-fraud-detection/'\n",
    "\n",
    "# sample_submission_df = pd.read_csv(f'{dataset_path}sample_submission.csv')\n",
    "\n",
    "train_transaction_df = pd.read_csv(f'{dataset_path}train_transaction.csv')\n",
    "test_transaction_df = pd.read_csv(f'{dataset_path}test_transaction.csv')\n",
    "\n",
    "train_id_df = pd.read_csv(f'{dataset_path}train_identity.csv')\n",
    "test_id_df = pd.read_csv(f'{dataset_path}test_identity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing our raw datasets, they need to be merged by TransactionID's.\n",
    "\n",
    "<b>(Note: throughout this notebook, it will be important to delete any dataframes that will not be used again as they take up space in memory and slow our machine down.)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T01:47:39.994659Z",
     "start_time": "2019-08-18T01:46:26.246819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = train_transaction_df.merge(train_id_df, on='TransactionID', how='left')\n",
    "test = test_transaction_df.merge(test_id_df, on='TransactionID', how='left')\n",
    "\n",
    "# Renaming columns for better description\n",
    "names = {\n",
    "    'addr1': 'billing zipcode',\n",
    "    'addr2': 'country codes',\n",
    "    'P_emaildomain': 'Purchaser_emaildom',\n",
    "    'R_emaildomain': 'Retailer_email.dom'\n",
    "}\n",
    "\n",
    "train.rename(columns=names, inplace=True)\n",
    "test.rename(columns=names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T01:47:41.024149Z",
     "start_time": "2019-08-18T01:47:40.022112Z"
    }
   },
   "outputs": [],
   "source": [
    "del train_transaction_df, train_id_df, test_transaction_df, test_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T01:47:41.247569Z",
     "start_time": "2019-08-18T01:47:41.032537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 590540 rows and 434 columns\n",
      "Training data has 506691 rows and 433 columns\n"
     ]
    }
   ],
   "source": [
    "print(f'Training data has {train.shape[0]} rows and {train.shape[1]} columns')\n",
    "print(f'Training data has {test.shape[0]} rows and {test.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T01:47:41.545083Z",
     "start_time": "2019-08-18T01:47:41.256465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>...</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>samsung browser 6.2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2220x1080</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>mobile</td>\n",
       "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3       card4  card5  ...                id_31  id_32  \\\n",
       "0    NaN  150.0    discover  142.0  ...                  NaN    NaN   \n",
       "1  404.0  150.0  mastercard  102.0  ...                  NaN    NaN   \n",
       "2  490.0  150.0        visa  166.0  ...                  NaN    NaN   \n",
       "3  567.0  150.0  mastercard  117.0  ...                  NaN    NaN   \n",
       "4  514.0  150.0  mastercard  102.0  ...  samsung browser 6.2   32.0   \n",
       "\n",
       "       id_33           id_34  id_35 id_36 id_37  id_38  DeviceType  \\\n",
       "0        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "1        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "2        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "3        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "4  2220x1080  match_status:2      T     F     T      T      mobile   \n",
       "\n",
       "                      DeviceInfo  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4  SAMSUNG SM-G892A Build/NRD90M  \n",
       "\n",
       "[5 rows x 434 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:01:07.312403Z",
     "start_time": "2019-08-16T03:01:07.246445Z"
    }
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "Visually exploring the dataset it important for familiarizing ourselves with it and trying to better understand the data before complex analysis is undertaken.\n",
    "\n",
    "### Data\n",
    "First, lets get an overview of the features and their data types.\n",
    "\n",
    "<b>(Note: due to the large output the following script produces, the output can be found in the eda_output folder in under feature_overview.txt)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:01:26.144837Z",
     "start_time": "2019-08-16T03:01:07.317967Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is for local environment\n",
    "# eda_output_path = 'eda_output/'\n",
    "# feat_over_file = 'feature_overview.txt'\n",
    "\n",
    "# if os.path.exists(f'{eda_output_path}{feat_over_file}'):\n",
    "#     os.remove(f'{eda_output_path}{feat_over_file}')\n",
    "\n",
    "# with open(f'{eda_output_path}{feat_over_file}', 'a') as overview_file:\n",
    "#     for col, values in train.iteritems():\n",
    "#         overview_file.write(f'{col}: {values.nunique()} ({values.dtypes})\\n')\n",
    "#         overview_file.write(str(values.unique()[:100]))\n",
    "#         overview_file.write(\n",
    "#             '\\n\\n###########################################################\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we examine the feature_overview.txt file, we notice that there are many continuous features and some categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:01:48.846804Z",
     "start_time": "2019-08-16T03:01:26.151144Z"
    }
   },
   "outputs": [],
   "source": [
    "train_contains_na = train.isna().any().sum()\n",
    "test_contains_na = test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:01:48.868948Z",
     "start_time": "2019-08-16T03:01:48.850805Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'{train_contains_na} out of {len(train.columns)} columns contain missing values in the train data')\n",
    "print(f'{test_contains_na} out of {len(test.columns)} columns contain missing values in the test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:01:58.105123Z",
     "start_time": "2019-08-16T03:01:48.871997Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating percentage of missing values in each column\n",
    "train_missing_values = train.isna().mean().round(2)\n",
    "test_missing_values = test.isna().mean().round(2)\n",
    "\n",
    "# Keeping only columns that contain more than 50% missing values\n",
    "train_missing_values_5 = train_missing_values[train_missing_values.values > 0.05]\n",
    "test_missing_values_5 = test_missing_values[test_missing_values.values > 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:01:58.131725Z",
     "start_time": "2019-08-16T03:01:58.113345Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'{len(train_missing_values_5)} out of {len(train.columns)} columns in the train data contain more than 5% missing values')\n",
    "print(f'{len(test_missing_values_5)} out of {len(test.columns)} columns in the test data contain more than 5% missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are a large number of missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:01:58.183896Z",
     "start_time": "2019-08-16T03:01:58.138418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keeping only columns that contain more than 50% missing values\n",
    "train_missing_values_50 = train_missing_values[train_missing_values.values > 0.5]\n",
    "test_missing_values_50 = test_missing_values[test_missing_values.values > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:01:58.217669Z",
     "start_time": "2019-08-16T03:01:58.192341Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'{len(train_missing_values_50)} out of {len(train.columns)} columns in the train data contain more than 50% missing values')\n",
    "print(f'{len(test_missing_values_50)} out of {len(test.columns)} columns in the test data contain more than 50% missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Feature: isFraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:01:58.250972Z",
     "start_time": "2019-08-16T03:01:58.225003Z"
    }
   },
   "outputs": [],
   "source": [
    "isFraud = 'isFraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:01:58.317003Z",
     "start_time": "2019-08-16T03:01:58.258251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Function\n",
    "def PlotFunction(df, feature, title, xLable, yLabel, vertical=False, percentLabels=False, size=[10, 7]):\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    f = plt.figure(figsize=size)\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "    ax.title.set_text(title)\n",
    "    ax.set_ylabel(yLabel)\n",
    "    ax.set_xlabel(xLable)\n",
    "\n",
    "    plot = ax.bar([str(i) for i in df[feature].value_counts(dropna=False, normalize=True).index],\n",
    "                  df[feature].value_counts(dropna=False, normalize=True), 0.40,\n",
    "                  color=['cornflowerblue', 'darkorange', 'green', 'brown', 'black'])\n",
    "    if vertical:\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "    # Add counts above the two bar graphs\n",
    "    if percentLabels:\n",
    "        percentages = (df[feature].value_counts(\n",
    "            dropna=False, normalize=True)*100).round(3)\n",
    "        i = 0\n",
    "        for rect in plot:\n",
    "            height = rect.get_height()\n",
    "            plt.text(rect.get_x() + rect.get_width()/2.0, height,\n",
    "                     f'{percentages[i]}%', ha='center', va='bottom')\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:01:58.389324Z",
     "start_time": "2019-08-16T03:01:58.325342Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'These are the two types of values for fraud: {train[isFraud].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:01:58.438819Z",
     "start_time": "2019-08-16T03:01:58.400059Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'isFraud contains {train[isFraud].isna().any() * 1} missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:00.024680Z",
     "start_time": "2019-08-16T03:01:58.447845Z"
    }
   },
   "outputs": [],
   "source": [
    "### 'Fraud' Feature\n",
    "PlotFunction(train, isFraud, 'isFaud percentages', 'Not fraud | Fraud', 'Number of occurances', percentLabels=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visual examination of the graph reveals an imbalance in the number of fraudulent transactions.\n",
    "\n",
    "To correct this imbalanace, the use of a sampling method such as SMOTE or oversampling will be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:01.718525Z",
     "start_time": "2019-08-16T03:02:00.036968Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'TransactionAmt' feature\n",
    "plt.figure(figsize=[12, 5])\n",
    "plt.title('Transaction Amounts')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Number of transactions')\n",
    "_ = plt.hist(train['TransactionAmt'], bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear right skew to this data. I will try a log transformation to reduce this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:03.586832Z",
     "start_time": "2019-08-16T03:02:01.725745Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'TransactionAmt' feature\n",
    "plt.figure(figsize=[12, 5])\n",
    "plt.title('Transaction Amounts')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Number of transactions')\n",
    "_ = plt.hist(train['TransactionAmt'].apply(np.log), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'TransactionAmt' now shows a normal distribution, we can separate which transactions were fraudulent and compare whether they are larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:05.689164Z",
     "start_time": "2019-08-16T03:02:03.605683Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'TransactionAmt' feature\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 3))\n",
    "\n",
    "# Not Fraud\n",
    "ax1.title.set_text('Non-Fraudulent Transaction Amounts')\n",
    "ax1.set_xlabel('LgAmount')\n",
    "ax1.set_ylabel('Number of transactions')\n",
    "_ = ax1.hist(train['TransactionAmt']\n",
    "             [train['isFraud'] == 0].apply(np.log), bins=100)\n",
    "\n",
    "# Fraud\n",
    "ax2.title.set_text('Fraudulent Transaction Amounts')\n",
    "ax2.set_xlabel('LgAmount')\n",
    "ax2.set_ylabel('Number of transactions')\n",
    "_ = ax2.hist(train['TransactionAmt']\n",
    "             [train['isFraud'] == 1].apply(np.log), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see that the fraudulent transaction amounts seem to be higher on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:06.632967Z",
     "start_time": "2019-08-16T03:02:05.695559Z"
    }
   },
   "outputs": [],
   "source": [
    "### 'ProductCD' feature\n",
    "PlotFunction(train, 'ProductCD', 'Product codes for each transaction', 'Product Codes', 'Count', percentLabels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:07.178147Z",
     "start_time": "2019-08-16T03:02:06.638193Z"
    }
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=[15, 5])\n",
    "ax = f.add_subplot(1, 1, 1)\n",
    "ax.title.set_text('Percentage of products from fraudulent transactions')\n",
    "ax.set_ylabel('Percent')\n",
    "ax.set_xlabel('Products')\n",
    "\n",
    "plot = ax.bar([str(i) for i in train['ProductCD'][train['isFraud'] == 1].value_counts(dropna=False, normalize=True).index],\n",
    "              train['ProductCD'][train['isFraud'] == 1].value_counts(\n",
    "                  dropna=False, normalize=True), 0.40,\n",
    "              color=['cornflowerblue', 'darkorange', 'green', 'brown', 'black'])\n",
    "\n",
    "# Add counts above the two bar graphs\n",
    "percentages = (train['ProductCD'][train['isFraud'] == 1].value_counts(\n",
    "    dropna=False, normalize=True)*100).round(3)\n",
    "i = 0\n",
    "for rect in plot:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2.0, height,\n",
    "             f'{percentages[i]}%', ha='center', va='bottom')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main products involved with fraululent transactions, 'W' and 'C'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:07.316368Z",
     "start_time": "2019-08-16T03:02:07.183698Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'card1' feature\n",
    "print('Number of unique values:', len(train['card1'].unique()))\n",
    "train['card1'].value_counts(dropna=False, normalize=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:07.406053Z",
     "start_time": "2019-08-16T03:02:07.322346Z"
    }
   },
   "outputs": [],
   "source": [
    "### 'card2' feature\n",
    "print('Number of unique values:', len(train['card2'].unique()))\n",
    "train['card2'].value_counts(dropna=False, normalize=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'card1' & 'card2 have a large number of unique values with no clear dominating catagory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:07.480387Z",
     "start_time": "2019-08-16T03:02:07.411423Z"
    }
   },
   "outputs": [],
   "source": [
    "### 'card3' feature\n",
    "print('Number of unique values:', len(train['card3'].unique()))\n",
    "train['card3'].value_counts(dropna=False, normalize=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'card3' has 88% of its values being 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:07.546464Z",
     "start_time": "2019-08-16T03:02:07.486621Z"
    }
   },
   "outputs": [],
   "source": [
    "### 'card5' feature\n",
    "print('Number of unique values:', len(train['card5'].unique()))\n",
    "train['card5'].value_counts(dropna=False, normalize=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'card5' has 50% of its values being 226."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:08.598207Z",
     "start_time": "2019-08-16T03:02:07.552506Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### 'card4' feature\n",
    "PlotFunction(train, 'card4', 'Number of card types', 'Card types', 'Number of card occurrences', percentLabels=True)\n",
    "\n",
    "print('Number of unique values:', len(train['card4'].unique()))\n",
    "train['card4'].value_counts(dropna=False, normalize=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'visa' cards are have the highest use in this dataset at 65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:09.541184Z",
     "start_time": "2019-08-16T03:02:08.600643Z"
    }
   },
   "outputs": [],
   "source": [
    "### 'card6' feature\n",
    "PlotFunction(train, 'card6', 'Number of account types', 'Account types', 'Number of account type occurrences', percentLabels=True)\n",
    "\n",
    "print('Number of unique values:', len(train['card6'].unique()))\n",
    "train['card6'].value_counts(dropna=False, normalize=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'debit' account types have the highest use in this dataset at 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:09.608682Z",
     "start_time": "2019-08-16T03:02:09.545267Z"
    }
   },
   "outputs": [],
   "source": [
    "### 'billing zipcode' feature\n",
    "print('Number of unique values:', len(train['billing zipcode'].unique()))\n",
    "train['billing zipcode'].value_counts(dropna=False, normalize=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:09.697359Z",
     "start_time": "2019-08-16T03:02:09.614323Z"
    }
   },
   "outputs": [],
   "source": [
    "### 'country codes' feature\n",
    "print('Number of unique values:', len(train['country codes'].unique()))\n",
    "train['country codes'].value_counts(dropna=False, normalize=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that 88% of transactions have come from <a href='https://en.wikipedia.org/wiki/List_of_UIC_country_codes'>France</a>. 11% of transactions that do not have a country code, however, it is unlikely that all of these transactions belong to the one country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:12.498200Z",
     "start_time": "2019-08-16T03:02:09.703097Z"
    }
   },
   "outputs": [],
   "source": [
    "### 'Purchaser Email' feature\n",
    "PlotFunction(train, 'Purchaser_emaildom', 'Number of Purchaser email types', 'Email types',\n",
    "             'Number of email type occurrences', vertical=True, percentLabels=False)\n",
    "\n",
    "print('Number of unique values:', len(train['Purchaser_emaildom'].unique()))\n",
    "train['Purchaser_emaildom'].value_counts(dropna=False, normalize=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'gmail.com' accounts for 38% of the emails tied with purchaser transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:15.362561Z",
     "start_time": "2019-08-16T03:02:12.502685Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'Purchaser Email' feature\n",
    "PlotFunction(train, 'Retailer_email.dom', 'Number of Purchaser email types', 'Email types',\n",
    "             'Number of email type occurrences', vertical=True, percentLabels=False)\n",
    "\n",
    "print('Number of unique values:', len(train['Purchaser_emaildom'].unique()))\n",
    "train['Retailer_email.dom'].value_counts(dropna=False, normalize=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'gmail.com' accounts for 77% of the emails tied with retailer transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:15.373820Z",
     "start_time": "2019-08-16T03:02:15.366225Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_names = ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:15.419786Z",
     "start_time": "2019-08-16T03:02:15.376117Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_names = ['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:19.505222Z",
     "start_time": "2019-08-16T03:02:15.424165Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_names = ['M1', 'M2', 'M4', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9']\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(17, 10))\n",
    "\n",
    "j = 0\n",
    "for row in axes:\n",
    "    for ax in row:\n",
    "        ax.set_title(feature_names[j])\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_xlabel(feature_names[j])\n",
    "\n",
    "        plot = ax.bar([str(i) for i in train[feature_names[j]].value_counts(dropna=False, normalize=True).index],\n",
    "                      train[feature_names[j]].value_counts(\n",
    "                          dropna=False, normalize=True), 0.40,\n",
    "                      color=['cornflowerblue', 'darkorange', 'green', 'brown', 'black'])\n",
    "        j += 1\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:19.519170Z",
     "start_time": "2019-08-16T03:02:19.509301Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_names = ['id_12', 'id_15', 'id_16', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "Due to the large number of NaN values our dataframes contain, it is critical that they are replaced with a meaningful placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T01:47:45.207369Z",
     "start_time": "2019-08-18T01:47:41.551685Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = train['isFraud'].copy()\n",
    "X_train = train.drop('isFraud', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features that consist of more than 5% missing values will be excluded as the potential effect of bias may become to large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T01:47:45.231202Z",
     "start_time": "2019-08-18T01:47:45.221102Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_missing_values = train_missing_values[train_missing_values.values > 0.20]\n",
    "# test_missing_values = test_missing_values[test_missing_values.values > 0.20]\n",
    "\n",
    "# test.drop(test_missing_values.index, axis=1, inplace=True)\n",
    "# X_train.drop(train_missing_values.index, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent our model from crashing, it is important to replace missing values with so derived value. The replacement values for categorical features will be the mode of that feature, and for numerical features, it will be the mean for that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T01:48:11.731843Z",
     "start_time": "2019-08-18T01:47:45.239377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for feature in X_train.columns:\n",
    "    if X_train[feature].isna().any():\n",
    "        if X_train[feature].dtype == 'object':\n",
    "            X_train[feature] = X_train[feature].fillna(X_train[feature].mode())\n",
    "            test[feature] = test[feature].fillna(test[feature].mode())\n",
    "        else:\n",
    "            X_train[feature] = X_train[feature].fillna(X_train[feature].mean())\n",
    "            test[feature] = test[feature].fillna(test[feature].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our model can not accept categorical features as object data types, a label transformation must be preformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T01:49:56.907082Z",
     "start_time": "2019-08-18T01:48:11.764670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for feature in X_train.columns:\n",
    "    if X_train[feature].dtype == 'object' or test[feature].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(X_train[feature].values) + list(test[feature].values))\n",
    "        X_train[feature] = le.transform(list(X_train[feature].values))\n",
    "        test[feature] = le.transform(list(test[feature].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE or Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "As we are preforming mean different feature selection methods, it is important to store each set of features for testing.\n",
    "\n",
    "First up, we will append all of our features before selection into our feature sets dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.093487Z",
     "start_time": "2019-08-16T02:57:26.956Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature_sets = {}\n",
    "# feature_sets['all'] = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.096507Z",
     "start_time": "2019-08-16T02:57:26.963Z"
    },
    "scrolled": true
   },
   "source": [
    "#### Random Forest Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build classifier\n",
    "# clf = RandomForestClassifier(n_estimators=250, max_depth=1, random_state=42)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Select features > 0\n",
    "# importances = pd.DataFrame(X_train.columns, columns=['features'])\n",
    "# importances['importance'] = clf.feature_importances_\n",
    "# importances.sort_values(by='importance', axis=0, ascending=False, inplace=True)\n",
    "# importances = importances[importances['importance'] > 0]\n",
    "\n",
    "# # Plot best features\n",
    "# plt.figure(figsize=[20,10])\n",
    "# plt.title(\"Feature importances\")\n",
    "# plt.bar(range(len(importances)), importances['importance'])\n",
    "# plt.xticks(range(len(importances)), importances['features'])\n",
    "# plt.xlim([-1, len(importances)])\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.102613Z",
     "start_time": "2019-08-16T02:57:26.979Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature_sets['random_forest'] = importances['features'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.107191Z",
     "start_time": "2019-08-16T02:57:26.986Z"
    }
   },
   "source": [
    "#### Chi Squared Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.112949Z",
     "start_time": "2019-08-16T02:57:26.993Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Scale features to avoid negative values crashing chi2\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# # Building the model\n",
    "# bestfeatures = SelectKBest(score_func=chi2, k=2)\n",
    "# fit = bestfeatures.fit(X_train_scaled, y_train.values)\n",
    "\n",
    "# # Select features > 50\n",
    "# feature_scores = pd.DataFrame(X_train.columns, columns=['features'])\n",
    "# feature_scores['scores'] = fit.scores_\n",
    "# feature_scores.sort_values(by='scores', axis=0, ascending=False, inplace=True)\n",
    "# feature_score = feature_scores[feature_scores['scores']>50]\n",
    "\n",
    "# # Plot best features\n",
    "# plt.figure(figsize=[20,10])\n",
    "# plt.title(\"Feature importances\")\n",
    "# plt.bar(range(len(feature_scores)), feature_scores['scores'])\n",
    "# plt.xticks(range(len(feature_scores)), feature_scores['features'])\n",
    "# plt.xlim([-1, len(feature_scores)])\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.117227Z",
     "start_time": "2019-08-16T02:57:27.000Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature_sets['chi2'] = feature_score['features'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.120861Z",
     "start_time": "2019-08-16T02:57:27.008Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T08:24:07.126638Z",
     "start_time": "2019-08-15T08:24:07.103417Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.124823Z",
     "start_time": "2019-08-16T02:57:27.032Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T03:06:46.760768Z",
     "start_time": "2019-08-18T03:06:46.504643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96500999, 0.96500999])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def calculate_decision_stump_efficient1(data, weight, label):\n",
    "\n",
    "x2 = X_train[['TransactionDT','TransactionAmt']].values\n",
    "weight2 = np.ones((x2.shape[0], x2.shape[1]), dtype=np.float64)/len(x2)\n",
    "label2 = y_train.values\n",
    "pos2 = np.where(label2 == 0)\n",
    "label2[pos2] = -1\n",
    "# calculate_decision_stump_efficient1(x2, weight2, label2)\n",
    "\n",
    "\n",
    "Tp=np.float64(0) #T+ total sum of positive examples weights\n",
    "Tn=np.float64(0) #T- total sum of negative examples weights\n",
    "Sp=np.float64(0) #S+ sum of positive weights below the cuurent threshold\n",
    "Sn=np.float64(0) #S- sum of negative weights below the current threshold\n",
    "error1=np.float64(0)\n",
    "error2=np.float64(0)\n",
    "min_error=np.float64(2.0) \n",
    "min_thresh=np.float64(0) \n",
    "direction=1\n",
    "\n",
    "#     y = np.zeros(N, dtype=np.int64)\n",
    "\n",
    "#get all positive weights    \n",
    "temp = (label2 == 1)\n",
    "temp = np.reshape(np.int64(temp),(-1,1))\n",
    "Tp = np.sum((temp * weight2), axis=0)\n",
    "\n",
    "# get all negative weights  \n",
    "temp  = (label2 == -1)\n",
    "temp = np.reshape(np.int64(temp),(-1,1))\n",
    "Tn = np.sum((temp * weight2), axis=0)\n",
    "\n",
    "\n",
    "Sn = np.sum(weight2[np.where(label2 == -1)[0]], axis=0)\n",
    "Sp = np.sum(weight2[np.where(label2 == 1)[0]], axis=0)\n",
    "\n",
    "#sort feature values\n",
    "# sorted_labels = data[:, feature].argsort()\n",
    "# sorted_vector =  data[sorted_labels]\n",
    "# neg_weights = weight2[np.where(label2 == -1)[0]]\n",
    "# pos_weights = weight2[np.where(label2 == 1)[0]]\n",
    "\n",
    "# Sn = np.zeros((neg_weights.shape[0], neg_weights.shape[1]))\n",
    "# Sp = np.zeros((pos_weights.shape[0], pos_weights.shape[1]))\n",
    "\n",
    "\n",
    "# error1 = Sp + (Tn - Sn)\n",
    "# print((Tn - Sn))\n",
    "# print(Tn)\n",
    "\n",
    "# i = -1\n",
    "# for row in neg_weights:\n",
    "#     Sn[i + 1] = Sn[i] + row\n",
    "#     i += 1\n",
    "\n",
    "# i = -1\n",
    "# for x in pos_weights:\n",
    "#     Sp[i + 1] = Sp[i] + row\n",
    "#     i += 1\n",
    "\n",
    "\n",
    "# error2 = Sn + (Tp - Sp) \n",
    "\n",
    "# print(error1)\n",
    "\n",
    "# if(min_error > error1) :\n",
    "#     min_error = error1\n",
    "#     min_thresh = sorted_vector[i, feature]\n",
    "#     direction = 1\n",
    "# if(min_error > error2) :\n",
    "#     min_error = error2\n",
    "#     min_thresh = sorted_vector[i, feature]\n",
    "#     direction = -1   \n",
    "    \n",
    "    \n",
    "# length = len(sorted_vector)\n",
    "# for i in range(length):\n",
    "\n",
    "#     #RIGHT DIRECTION THRESHOLD\n",
    "#     #error1 is the sum of positives up to that point + total negatives minus the sum of negatives so far\n",
    "#     error1 = Sp + (Tn - Sn) \n",
    "#     if label[i] == -1 : \n",
    "#         Sn = Sn +  weight[i]\n",
    "#     else :\n",
    "#         Sp = Sp + weight[i]\n",
    "\n",
    "#     #LEFT DIRECTION THRESHOLD\n",
    "#     error2 = Sn + (Tp - Sp) \n",
    "\n",
    "#     if(min_error > error1) :\n",
    "#         min_error = error1\n",
    "#         min_thresh = sorted_vector[i, feature]\n",
    "#         direction = 1\n",
    "#     if(min_error > error2) :\n",
    "#         min_error = error2\n",
    "#         min_thresh = sorted_vector[i, feature]\n",
    "#         direction = -1           \n",
    "\n",
    "# return min_thresh, direction, min_error\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T00:00:29.244236Z",
     "start_time": "2019-08-18T00:00:24.864510Z"
    }
   },
   "outputs": [],
   "source": [
    "x = X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T00:00:37.117344Z",
     "start_time": "2019-08-18T00:00:32.083267Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_labels = x[:, 0].argsort()\n",
    "sorted_vector =  x[sorted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.values.argsort().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T00:08:16.074921Z",
     "start_time": "2019-08-18T00:08:16.057547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 6])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = np.array([1,2,3])\n",
    "j = np.array([1,2,3])\n",
    "i + j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.137258Z",
     "start_time": "2019-08-16T02:57:27.066Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def calculate_decision_stump_efficient(data, label, feature, weight, N):\n",
    "\n",
    "    Tp=np.float64(0); #T+ total sum of positive examples weights\n",
    "    Tn=np.float64(0) #T- total sum of negative examples weights\n",
    "    Sp=np.float64(0) #S+ sum of positive weights below the cuurent threshold\n",
    "    Sn=np.float64(0) #S- sum of negative weights below the current threshold\n",
    "    error1=np.float64(0)\n",
    "    error2=np.float64(0)\n",
    "    min_error=np.float64(2.0) \n",
    "    min_thresh=np.float64(0) \n",
    "    direction=1\n",
    "    \n",
    "    y = np.zeros(N, dtype=np.int64)\n",
    "    \n",
    "    #get all positive weights    \n",
    "    temp  = (label == 1)\n",
    "    temp = np.int64(temp)\n",
    "    Tp = np.sum(temp * weight)\n",
    "    \n",
    "    #get all negative weights  \n",
    "    temp  = (label == -1)\n",
    "    temp = np.int64(temp)\n",
    "    Tn = np.sum(temp * weight)\n",
    "    \n",
    "    #sort feature values\n",
    "    sorted_labels = data[:, feature].argsort()\n",
    "    sorted_vector =  data[sorted_labels]\n",
    "\n",
    "    length = len(sorted_vector)\n",
    "    for i in range(length):\n",
    "\n",
    "        #RIGHT DIRECTION THRESHOLD\n",
    "        #error1 is the sum of positives up to that point + total negatives minus the sum of negatives so far\n",
    "        error1 = Sp + (Tn - Sn)\n",
    "        if label[sorted_labels[i]] == -1 : \n",
    "            Sn += weight[sorted_labels[i]]\n",
    "        else :\n",
    "            Sp += weight[sorted_labels[i]]\n",
    "            \n",
    "        #LEFT DIRECTION THRESHOLD\n",
    "        error2 = Sn + (Tp - Sp) \n",
    "        \n",
    "        if(min_error > error1) :\n",
    "            min_error = error1\n",
    "            min_thresh = sorted_vector[i, feature]\n",
    "            direction = 1\n",
    "        if(min_error > error2) :\n",
    "            min_error = error2\n",
    "            min_thresh = sorted_vector[i, feature]\n",
    "            direction = -1           \n",
    "    \n",
    "    return min_thresh, direction, min_error\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.141025Z",
     "start_time": "2019-08-16T02:57:27.074Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def classify_dataset_against_weak_classifier(x, thresh, direction):\n",
    "    if direction == -1:\n",
    "        return np.where(x < thresh, 1, -1)\n",
    "    else:\n",
    "        return np.where(x < thresh, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.144648Z",
     "start_time": "2019-08-16T02:57:27.081Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# X: an two dimensional np.array of independent features values.\n",
    "# Y: an one dimensional np.array of actual target values. Values either {-1,1}.\n",
    "# T: number of boosting arounds.\n",
    "# features: a list of feature names.\n",
    "# learn_rate: the learning rate used to calculate alpha[t].\n",
    "def fit(X, Y, T, features, learn_rate):\n",
    "    \n",
    "    h=np.zeros([T,3], dtype=np.float64)\n",
    "    alphas = np.zeros(T, dtype=np.float64)\n",
    "    err = np.ones(T, dtype=np.float64) * np.inf\n",
    "    weight=np.ones(len(X), dtype=np.float64)/len(X)\n",
    "    N = len(X)\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Iterate through every feature in our training data.\n",
    "        for feature in range(len(features)):\n",
    "            weighted_error = np.float64(0)\n",
    "\n",
    "            # Build our decision stump.\n",
    "            threshold, sign, weighted_error = calculate_decision_stump_efficient(X, Y, feature, weight, N)\n",
    "\n",
    "            # Select the best model and feature that produces the lowest model error\n",
    "            if weighted_error < err[t]:\n",
    "                err[t] = weighted_error\n",
    "                h[t][0] = threshold\n",
    "                h[t][1] = feature\n",
    "                h[t][2] = sign\n",
    "\n",
    "        # Calculate alpha value based\n",
    "        alphas[t] = learn_rate * np.log( (1.0 - err[t])/err[t])\n",
    "        \n",
    "        # Make a prediction using t decision stump classifier\n",
    "        classification = classify_dataset_against_weak_classifier(X[:, int(h[t][1])], h[t][0], h[t][2] )\n",
    "\n",
    "        # Update the weights based on what was not correctly, some alpha value and a learning rate\n",
    "        weight *= np.exp(-1.0*alphas[t]*classification*labels)\n",
    "        \n",
    "        # Normalize the weights distribution\n",
    "        weight = weight / np.sum(weight)\n",
    "        \n",
    "    return np.append(h,np.reshape(alphas,(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.146823Z",
     "start_time": "2019-08-16T02:57:27.089Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(data_arr, boost_classif):\n",
    "    \n",
    "    classification_arr = np.zeros((len(data_arr),1))\n",
    "    \n",
    "    for idx, thresh, feat, sign, alpha in boost_classif.itertuples():\n",
    "        ht1 = np.sign(data_arr[:, int(feat)] - thresh) * sign\n",
    "        classification_arr += alpha*np.reshape(ht1,(-1,1))\n",
    "\n",
    "    classification_arr = np.where(classification_arr >= 0,1,-1)\n",
    "    \n",
    "    return classification_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.151651Z",
     "start_time": "2019-08-16T02:57:27.097Z"
    }
   },
   "outputs": [],
   "source": [
    "def sum_classifier_votes_for_each_sample(dataset, classifier_df):\n",
    "    classification = np.float64(0)\n",
    "    neg = np.float64(0)\n",
    "    pos = np.float64(0)\n",
    "    for idx, thresh, feat, sign, alpha in classifier_df.itertuples():\n",
    "\n",
    "        ht1 = np.sign(dataset[:, int(feat)] - thresh) * sign\n",
    "        classification += alpha * ht1\n",
    "\n",
    "        neg += np.where(ht1 < 0, ht1, 0) * alpha\n",
    "        pos += np.where(ht1 >= 0, ht1, 0) * alpha\n",
    "\n",
    "    return classification, pos, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.154633Z",
     "start_time": "2019-08-16T02:57:27.107Z"
    }
   },
   "outputs": [],
   "source": [
    "def margin_calculation_for_training_samples(sign, pos, neg, tot_votes):\n",
    "    if np.sign(sign) < 0:\n",
    "        return np.abs(neg) / tot_votes, -1\n",
    "    else:\n",
    "        return pos / tot_votes, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.157918Z",
     "start_time": "2019-08-16T02:57:27.116Z"
    }
   },
   "outputs": [],
   "source": [
    "def sign_of_margin(margin, classification, true_class_label):\n",
    "    if classification != true_class_label:\n",
    "        return -1 * margin\n",
    "    else:\n",
    "        return margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.160366Z",
     "start_time": "2019-08-16T02:57:27.125Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(statistic_df, data):\n",
    "    return np.sum(np.where(statistic_df['classification'] != statistic_df['true_class_label'],1,0)) / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_measures(y_test, y_pred):\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print('Model accuracy: %f ' % accuracy)\n",
    "\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    print('Precision: %f' % precision)\n",
    "\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    print('Recall: %f' % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Selecting the features\n",
    "features = X_train.columns.tolist()\n",
    "# features = feature_sets['all']\n",
    "\n",
    "# Creating our np.array of features\n",
    "X_train_arr = X_train.values\n",
    "test_arr = test.values\n",
    "# del X_train, X_test\n",
    "\n",
    "# Creating our np.array of target values\n",
    "labels = y_train.values\n",
    "pos2 = np.where(labels == 0)\n",
    "labels[pos2] = -1\n",
    "# del y_train\n",
    "\n",
    "# Number of boosting rounds\n",
    "T=30\n",
    "\n",
    "# Alpha learning rate\n",
    "learning_rate = 0.5\n",
    "\n",
    "models_all = fit(X_train_arr, labels, T, features, learning_rate)\n",
    "\n",
    "classifier_df_all = pd.DataFrame(models_all, columns=['threshold','feature','direction','alpha'])\n",
    "\n",
    "y_pred_all = predict(test_arr, classifier_df_all)\n",
    "\n",
    "model_measures(y_test, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Selecting the features\n",
    "# # features = anova_result['features']\n",
    "# features = feature_sets['random_forest']\n",
    "\n",
    "# # Creating our np.array of features\n",
    "# X_train_arr_rf = X_train[features].values\n",
    "# test_arr = test[features].values\n",
    "# # del X_train, X_test\n",
    "\n",
    "# # Creating our np.array of target values\n",
    "# labels = y_train.values\n",
    "# pos2 = np.where(labels == 0)\n",
    "# labels[pos2] = -1\n",
    "# # del y_train\n",
    "\n",
    "# # Number of boosting rounds\n",
    "# T=50\n",
    "\n",
    "# # Alpha learning rate\n",
    "# learning_rate = 0.5\n",
    "\n",
    "# models_rf = fit(X_train_arr_rf, labels, T, features, learning_rate)\n",
    "\n",
    "# classifier_df_rf = pd.DataFrame(models_rf, columns=['threshold','feature','direction','alpha'])\n",
    "\n",
    "# y_pred_rf = predict(test_arr, classifier_df_rf)\n",
    "\n",
    "# model_measures(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Selecting the features\n",
    "# # features = anova_result['features']\n",
    "# features = feature_sets['chi2']\n",
    "\n",
    "# # Creating our np.array of features\n",
    "# X_train_arr_chi = X_train[features].values\n",
    "# test_arr = test[features].values\n",
    "# # del X_train, X_test\n",
    "\n",
    "# # Creating our np.array of target values\n",
    "# labels = y_train.values\n",
    "# pos2 = np.where(labels == 0)\n",
    "# labels[pos2] = -1\n",
    "# # del y_train\n",
    "\n",
    "# # Number of boosting rounds\n",
    "# T=50\n",
    "\n",
    "# # Alpha learning rate\n",
    "# learning_rate = 0.5\n",
    "\n",
    "# models_chi = fit(X_train_arr_chi, labels, T, features, learning_rate)\n",
    "\n",
    "# classifier_df_chi = pd.DataFrame(models_chi, columns=['threshold','feature','direction','alpha'])\n",
    "\n",
    "# y_pred_chi = predict(test_arr, classifier_df_chi)\n",
    "\n",
    "# model_measures(y_test, y_pred_chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_df_all = pd.DataFrame(test['TransactionID'])\n",
    "# finished_df_rf = pd.DataFrame(test['TransactionID'])\n",
    "# finished_df_chi = pd.DataFrame(test['TransactionID'])\n",
    "\n",
    "finished_df_all['isFraud'] = np.where(y_pred_all == -1,0,1)\n",
    "# finished_df_rf['isFraud'] = np.where(y_pred_rf == -1,0,1)\n",
    "# finished_df_chi['isFraud'] = np.where(y_pred_chi == -1,0,1)\n",
    "\n",
    "\n",
    "finished_df_all.to_csv('predictions_all.csv', index=False)\n",
    "# finished_df_rf.to_csv('predictions_rf.csv', index=False)\n",
    "# finished_df_chi.to_csv('predictions_chi.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.163414Z",
     "start_time": "2019-08-16T02:57:27.134Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# # Selecting the features\n",
    "# # features = anova_result['features']\n",
    "# features = X_train.columns[:2]\n",
    "\n",
    "# # Creating our np.array of features\n",
    "# X_train_arr = X_train[features].values\n",
    "# test_arr = test[features].values\n",
    "# # del X_train, X_test\n",
    "\n",
    "# # Creating our np.array of target values\n",
    "# labels = y_train.values\n",
    "# pos1 = np.nonzero(labels == 1)\n",
    "# pos2 = np.where(labels == 0)\n",
    "# labels[pos2] = -1\n",
    "# # del y_train\n",
    "\n",
    "# # Number of boosting rounds\n",
    "# T=50\n",
    "\n",
    "# # Alpha learning rate\n",
    "# learning_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.166621Z",
     "start_time": "2019-08-16T02:57:27.143Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# models = fit(X_train_arr, labels, T, features, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.168338Z",
     "start_time": "2019-08-16T02:57:27.150Z"
    }
   },
   "outputs": [],
   "source": [
    "# classifier_df = pd.DataFrame(models, columns=['threshold','feature','direction','alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.172321Z",
     "start_time": "2019-08-16T02:57:27.159Z"
    }
   },
   "outputs": [],
   "source": [
    "# y_pred = predict(X_test, classifier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.176196Z",
     "start_time": "2019-08-16T02:57:27.167Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Here we test how accurate our model is by using the test data: X_test_arr\n",
    "# statistics_df = pd.DataFrame(X_train_arr)\n",
    "# sum_alpha, pos_votes, neg_votes = sum_classifier_votes_for_each_sample(X_train_arr, classifier_df)\n",
    "# statistics_df['sum_alpha'] = sum_alpha\n",
    "# statistics_df['pos_votes'] = pos_votes\n",
    "# statistics_df['neg_votes'] = neg_votes\n",
    "\n",
    "\n",
    "# statistics_df['total_alpha_votes'] = np.sum(classifier_df.alpha)\n",
    "# result = statistics_df[['sum_alpha','pos_votes','neg_votes','total_alpha_votes']].apply(lambda x: margin_calculation_for_training_samples(*x), axis=1)\n",
    "# statistics_df['margin'] = result.apply(lambda x: x[0])\n",
    "# statistics_df['classification'] = result.apply(lambda x: x[1])\n",
    "# statistics_df['true_class_label'] = labels\n",
    "\n",
    "# print(f'Model accuracy: {metrics.accuracy_score(y_train, statistics_df[\"classification\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.179342Z",
     "start_time": "2019-08-16T02:57:27.176Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.182461Z",
     "start_time": "2019-08-16T02:57:27.188Z"
    }
   },
   "outputs": [],
   "source": [
    "# y_pred = predict(test_arr, classifier_df)\n",
    "\n",
    "# # result_path = 'results/'\n",
    "# predictions_file = 'predictions.csv'\n",
    "\n",
    "# finished_df = pd.DataFrame(test['TransactionID'])\n",
    "# finished_df['isFraud'] = np.where(y_pred == -1,0,1)\n",
    "\n",
    "\n",
    "# finished_df.to_csv(predictions_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the margins and classifying the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our ensemble of weak classifiers and the alphas for each boosting round, we can begin to measure how confident we are in our models generalizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.188624Z",
     "start_time": "2019-08-16T02:57:27.213Z"
    }
   },
   "outputs": [],
   "source": [
    "# def plot_margin(data, labels, T, features, learning_rate,):\n",
    "    \n",
    "#     sign_name = 'sign_of_margin_' + str(T) + 'T'\n",
    "\n",
    "#     models = fit(data,labels,T,features,learning_rate)\n",
    "    \n",
    "#     classifier_df = pd.DataFrame(models, columns=['threshold','feature','direction','alpha'])\n",
    "\n",
    "\n",
    "#     statistics_df = pd.DataFrame(X_train_arr)\n",
    "#     sum_alpha, pos_votes, neg_votes = sum_classifier_votes_for_each_sample(data, classifier_df)\n",
    "#     statistics_df['sum_alpha'] = sum_alpha\n",
    "#     statistics_df['pos_votes'] = pos_votes\n",
    "#     statistics_df['neg_votes'] = neg_votes\n",
    "\n",
    "\n",
    "#     statistics_df['total_alpha_votes'] = np.sum(classifier_df.alpha)\n",
    "#     result = statistics_df[['sum_alpha','pos_votes','neg_votes','total_alpha_votes']].apply(lambda x: margin_calculation_for_training_samples(*x), axis=1)\n",
    "#     statistics_df['margin'] = result.apply(lambda x: x[0])\n",
    "#     statistics_df['classification'] = result.apply(lambda x: x[1])\n",
    "#     statistics_df['true_class_label'] = labels\n",
    "\n",
    "\n",
    "#     statistics_df[sign_name] = statistics_df[['margin', 'classification', 'true_class_label']].apply(lambda x: sign_of_margin(*x), axis=1)\n",
    "    \n",
    "#     sns.kdeplot(statistics_df[sign_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.192336Z",
     "start_time": "2019-08-16T02:57:27.222Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 30 Boosting rounds\n",
    "# plot_margin(X_train_arr, labels, 30, features, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.194425Z",
     "start_time": "2019-08-16T02:57:27.235Z"
    }
   },
   "outputs": [],
   "source": [
    "# 50 Boosting rounds\n",
    "# plot_margin(X_train_arr, labels, 50, features, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.197747Z",
     "start_time": "2019-08-16T02:57:27.247Z"
    }
   },
   "outputs": [],
   "source": [
    "# 100 Boosting rounds\n",
    "# plot_margin(X_train_arr, labels, 100, features, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.201662Z",
     "start_time": "2019-08-16T02:57:27.258Z"
    }
   },
   "outputs": [],
   "source": [
    "# 250 Boosting rounds\n",
    "# plot_margin(X_train_arr, labels, 250, features, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T03:45:50.109403Z",
     "start_time": "2019-08-15T03:45:49.666269Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.204692Z",
     "start_time": "2019-08-16T02:57:27.338Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# features = ['TransactionAmt', 'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']\n",
    "\n",
    "\n",
    "# T=30\n",
    "# # h=np.zeros([T,3])\n",
    "\n",
    "# alpha = np.zeros(T, dtype=np.float64)\n",
    "\n",
    "# # err = np.ones(T, dtype=np.float64) * np.inf\n",
    "\n",
    "# weight=np.ones(len(X_train[features]), dtype=np.float64)/len(X_train[features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.208078Z",
     "start_time": "2019-08-16T02:57:27.350Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# # First implementation\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# for t in range(T):\n",
    "#     Tree_model = DecisionTreeClassifier(criterion=\"gini\",max_depth=1)\n",
    "#     model = Tree_model.fit(X_train[features],y_train,sample_weight=weight)\n",
    "    \n",
    "#     predictions = model.predict(X_train[features])\n",
    "    \n",
    "#     score = model.score(X_train[features],y_train)\n",
    "#     model_error = 1 - score\n",
    "    \n",
    "#     misclassified = np.where(predictions != y_train, 1, 0)\n",
    "    \n",
    "# #     evaluation = np.where(predictions == y_train, 1, 0)\n",
    "# #     accuracy = sum(evaluation/len(evaluation))\n",
    "\n",
    "# #     misclassified = np.where(predictions != y_train, 1, 0)\n",
    "# #     misclassifcation = sum(misclassified/len(misclassified))\n",
    "\n",
    "# #     model_error = sum(weights*misclassified)/sum(weights)\n",
    "    \n",
    "    \n",
    "#     alpha = np.log((1-model_error)/model_error)\n",
    "#     alphas.append(alpha)\n",
    "    \n",
    "    \n",
    "#     X_train_weights *= np.exp(-1*alpha*misclassified)\n",
    "#     X_train_weights = X_train_weights / np.sum(X_train_weights)\n",
    "    \n",
    "    \n",
    "#     print(f'The Accuracy of the {t+1}. model is : {(score*100).round(3)}%')\n",
    "#     print(f'The misclassification rate is: {(model_error*100).round(3)}%')\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.211937Z",
     "start_time": "2019-08-16T02:57:27.364Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# features = anova_result['features'].tolist()\n",
    "\n",
    "\n",
    "# T = 50\n",
    "# learning_rate = 0.5\n",
    "\n",
    "# weights = np.ones(len(X_train[features]),\n",
    "#                   dtype=np.float64)/len(X_train[features])\n",
    "\n",
    "\n",
    "# alphas = []\n",
    "# models = []\n",
    "\n",
    "# for t in range(T):\n",
    "#     Tree_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=1)\n",
    "\n",
    "#     model = Tree_model.fit(X_train[features], y_train, sample_weight=weights)\n",
    "\n",
    "#     # Save your model\n",
    "#     models.append(model)\n",
    "\n",
    "#     predictions = model.predict(X_train[features])\n",
    "#     accuracy = model.score(X_train[features], y_train)\n",
    "\n",
    "#     # Seperate correctly classified from uncorrrectly classified\n",
    "# #     classified = np.array(predictions==y_train).astype(int)   # Not sure what this is used for\n",
    "#     misclassified = np.where(predictions != y_train,1,0)\n",
    "\n",
    "#     # Misclassification rate\n",
    "#     misclassification_rate = sum(misclassified)/len(misclassified)\n",
    "\n",
    "#     # Model error\n",
    "#     model_error = np.sum(weights*misclassified)/np.sum(weights)\n",
    "\n",
    "#     # Calculate the alpha values\n",
    "#     alpha = np.log((1-model_error)/model_error)\n",
    "#     # Save our alpha\n",
    "#     alphas.append(alpha)\n",
    "\n",
    "#     # Update the training weights for next decision stump\n",
    "#     weights *= np.exp(alpha*misclassified)\n",
    "\n",
    "#     # Standardize the weights\n",
    "# #     weights = weights / np.sum(weights)\n",
    "\n",
    "# #     Evaluation = pd.DataFrame(y_train.copy())\n",
    "# #     Evaluation['weights'] = weights\n",
    "# #     Evaluation['predictions'] = predictions\n",
    "# #     Evaluation['classified'] = classified\n",
    "# #     Evaluation['misclassified'] = misclassified\n",
    "\n",
    "# #     print(f'The Accuracy of the {t+1}th model is : {(accuracy*100).round(3)}%')\n",
    "# #     print(\n",
    "# #         f'The misclassification rate is: {(misclassification_rate * 100).round(3)}%')\n",
    "# #     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.217919Z",
     "start_time": "2019-08-16T02:57:27.376Z"
    }
   },
   "outputs": [],
   "source": [
    "# classification_sum = np.float64(0)\n",
    "\n",
    "# accuracy = []\n",
    "# total_predictions = []\n",
    "\n",
    "# for alpha, model in zip(alphas, models):\n",
    "#     model_predictions = model.predict(test[features])\n",
    "# #     print(model.score(X_test[features], y_test))\n",
    "#     total_predictions.append(model_predictions*alpha)\n",
    "    \n",
    "# total_predictions = np.sign(np.sum(np.array(total_predictions),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T03:02:26.220618Z",
     "start_time": "2019-08-16T02:57:27.383Z"
    }
   },
   "outputs": [],
   "source": [
    "# number_of_base_learners = 50\n",
    "# fig = plt.figure(figsize=(10,10))\n",
    "# ax0 = fig.add_subplot(111)\n",
    "# # for i in range(number_of_base_learners):\n",
    "# #     model = Boosting(dataset,i,dataset)\n",
    "# #     model.fit()\n",
    "# #     model.predict()\n",
    "# ax0.plot(range(len(accuracy)),accuracy,'-b')\n",
    "# ax0.set_xlabel('# models used for Boosting ')\n",
    "# ax0.set_ylabel('accuracy')\n",
    "# print('With a number of ',number_of_base_learners,'base models we receive an accuracy of ',accuracy[-1]*100,'%')    \n",
    "                 \n",
    "# plt.show()       "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
